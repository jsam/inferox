# DocLayout-YOLO Visual Inference Samples

This directory contains sample documents and the visual E2E inference pipeline.

## Structure

```
samples/
├── README.md                    # This file
├── 2410.12628v1.pdf            # Original PDF (DocLayout-YOLO paper)
├── prepare_samples.py          # Script to convert PDF → images
├── inputs/                     # Input images (generated from PDF)
│   ├── page_1.png
│   ├── page_2.png
│   └── page_3.png
└── outputs/                    # Output visualizations (generated by test)
    ├── page_1.png              # With bounding boxes
    ├── page_2.png
    └── page_3.png
```

## Quick Start

### 1. Prepare Input Images

Convert PDF pages to images:

```bash
cd samples
python prepare_samples.py
```

This creates PNG images in `inputs/` directory.

### 2. Run Visual E2E Test

Run the complete inference pipeline:

```bash
cd ..
export LIBTORCH_USE_PYTORCH=1
export LIBTORCH_BYPASS_VERSION_CHECK=1
export DYLD_LIBRARY_PATH=$(python -c "import torch; print(torch.__path__[0])")/lib:$DYLD_LIBRARY_PATH

cargo test --test visual_e2e_test -- --ignored --nocapture
```

This will:
1. Load images from `inputs/`
2. Preprocess images (resize to 1024×1024)
3. Run inference through Inferox Engine
4. Parse detections (bounding boxes + class labels)
5. Draw visualizations with colored boxes
6. Save results to `outputs/`

## Test Results

The test processes each page and detects document layout elements:

### Detected Classes

The model detects 10 different document elements:

1. **title** - Document titles, section headers (red)
2. **plain text** - Body text paragraphs (green)  
3. **abandon** - Edge elements, page numbers (blue)
4. **figure** - Images, charts, diagrams (yellow)
5. **figure_caption** - Figure captions (magenta)
6. **table** - Tables and tabular data (cyan)
7. **table_caption** - Table captions (dark red)
8. **table_footnote** - Footnotes within tables (dark green)
9. **isolate_formula** - Mathematical formulas (dark blue)
10. **formula_caption** - Formula captions (olive)

### Sample Detection Output

```
[1] Processing: page_1.png
    Original size: 1275×1650
    Model input: 1280×1280
    Found 10 detections (after NMS & filtering)
      1. plain text: 98.11% at (298, 482, 976, 920)
      2. plain text: 96.75% at (225, 1212, 1053, 1464)
      3. plain text: 96.40% at (223, 1017, 1053, 1202)
      4. title: 96.05% at (225, 161, 1050, 285)
      5. abandon: 94.02% at (250, 1481, 426, 1526)
      ... and 5 more
```

## Pipeline Details

### Preprocessing

- Load images from PNG files
- Resize to 1280×1280 (model input size)
- Normalize pixel values to [0, 1]
- Convert to RGB format
- Create batch tensor with shape `[1, 3, 1280, 1280]`

### Inference

- Uses Inferox Engine with TchBackend
- Model: DocLayout-YOLO (TorchScript)
- Output shape: `[1, 14, 33600]`
  - 14 features: [x, y, w, h, class0...class9]
  - 33,600 possible detections

### Postprocessing

- Parse tensor output to detections
- Apply class-specific confidence thresholds (0.4-0.6)
- Scale coordinates back to original image size
- Apply Non-Maximum Suppression (NMS) with IOU threshold 0.5
- Filter invalid detections (geometric constraints)
- Draw bounding boxes with class labels
- Save annotated images

## Output Visualization

Each output image shows:
- **Colored boxes** around detected elements
- **Class labels** with confidence scores
- **Original image** as background

The boxes are color-coded by class type for easy identification.

## Performance

On MacBook Pro M1 (CPU):
- **Preprocessing**: ~50ms per image
- **Inference**: ~3-4s per image
- **Postprocessing**: ~10ms per image
- **Total**: ~4s per image (including visualization)

## Requirements

- PyTorch 2.4.x (for LibTorch)
- Python packages: `pdf2image`, `pillow`
- Rust packages: `image`, `imageproc`, `ab_glyph`
- TorchScript model: `models/doclayout_yolo_torchscript.pt`

## Troubleshooting

### No input images

```bash
cd samples
python prepare_samples.py
```

### LibTorch not found

```bash
pip install 'torch<2.5'
export LIBTORCH_USE_PYTORCH=1
```

### Font rendering issues

The test uses the system font at `/Users/samuel.picek/inputlayer/inferox/assets/DejaVuSans.ttf`. If font loading fails, check that this file exists and is a valid TrueType font.

## Architecture

The visual E2E test demonstrates:

1. **Complete pipeline** from raw images to visualized results
2. **Inferox Engine integration** with backend-agnostic API
3. **TorchScript model** loading and inference
4. **Real-world document** processing (scientific paper PDF)
5. **Production-ready** visualization with labeled detections

This is a complete end-to-end example of using Inferox for document understanding tasks.
